# CSCI5502
Data Mining Final Project

## Scripts
### bst.py
A very simple Binary Search Tree implementation in Python. It's intended to be used along side `repo_filter.py` and `parser.py` to generate a tree of repo ids that will be considered when populating the database.

Note that to acieve the O(logn) search time when checking if an object exists in the tree, you must use the `contains(item)` method and not Python's `in` operator, as the latter will result in a call to the search tree's `__iter__()` method, which performs an in-order traversal on the tree, leading to an O(n) search.
### jsoncounter.py
Create a JSON file containing the number of times each repo appeared in the input file.

`python3 jsoncounter.py [(-o | --output) OUTFILE] FILE`

### jsonmerger.py
Combine the various individual count files created by `jsoncounter.py` into a single file, aggregating the counts.

`python3 jsonmerger.py [(-o | --output) OUTFILE] FILE1 FILE2...`

### repo_filter.py
This script makes use of `bst.py` to generate a Python3 pickle file containing a Binary Search Tree of repos that have an event frequency higher than a user supplied threshold. When run without any special arguments, the script will display basic statistics about the repo counts of the provided file.

`python3 repo_filter.py [(-t | --threshold) THRESH] [(-o | --output) NAME] FILE`
<table>
  <tr>
    <td> <strong> Option </strong> </td>
    <td> <strong> Meaning </strong> </td>
  </tr>
  <tr>
    <td> -t, --threshold THRESH </td>
    <td> Generate a dictionary pickle file caontaing the ids of each repo in the provided file that has at least THRESH events. </td>
  </tr>
  <tr>
    <td> -o, --output NAME </td>
    <td> Sepcify the name for the output pickle file. Ignored unless -b is specified. Defaults to the name of the input file followed by an underscore, the threshold vaule, and ".pickle" </td>
  </tr>
</table>

### parser.py
This script loads the events found in the specified .json.gz files into a MySQL database through the use of the MySQL Python Connector.

`python3 parser.py [(-u | --user) USER] [(-p | --pass)] [(-r | --repos) REPOS] GZIP_FILE1 GZIP_FILE2 ...`
<table>
  <tr>
    <td> <strong> Option </strong> </td>
    <td> <strong> Meaning </strong> </td>
  </tr>
  <tr>
    <td> -u, --user USER </td>
    <td> The username to use when connecting to the MySQL database. Defaults to the name of the user running the script. </td>
  </tr>
  <tr>
    <td> -p, --pass </td>
    <td> Specify that you wish to connect the MySQL database using a password. </td>
  </tr>
  <tr>
    <td> -r, --repos REPOS </td>
    <td> Specify a pickle file containing a Binary Search Tree of repo ids, as generated by <code>repo_filter.py</code>. Only events concerning repositories whose ids are contained within this tree will be added to the database. If not specified, no filtering is done based on repo, and all events contained in the input files are added to the database (if they are of a relevant type. i.e. Create, Push, Pull_Request, Issues)</td>
  </tr>
</table>

## Example Workflow
Download the compressed data from githubarchive.org using a simple `wget` command like the following:

`$ wget http://data.githubarchive.org/2015-01-01-{1..31}.json.gz`

Perform event counting on each downloaded file. This currently requires that the gzip file be extracted with `gzip -d FILE` or a similar command.

`$ python3 jsoncouter.py 2015-01-01-1.json`

Merge all intermediate count files together into a single file containing the aggregated counts.

`$ python3 jsonmerger.py -o jan_first.json *-out.json`

View some basic statistics about the total counts to help create a tree of repos.

`$ python3 repo_filter.py jan_first.json`

Generate the repo dict, containing repos that have an event count higher than some threshold, say 10 events.

`$ python3 repo_filter.py -t 10 -o jan_first_repos.pickle jan_first.json`

Populate the database using the dict of repos and bare gzip data files.

`$ python3 parser.py -u root -p -r jan_first_repos.pickle 2015-01-01-*.json.gz`

When performing queries on the database, it can be helpful to export data to files in order to perform post-processing such as visualization or analysis using Python. MySQL supports this (see https://dev.mysql.com/doc/refman/5.7/en/select-into.html for more info) with a simple command. Unfortunately, most likely, you can't export to any place on the filesystem, thanks to a `secure_file_priv` server variable. If you can figure out how to disable this easily, feel free to update this, but by default, it only allows files to be exported to a specific directory (`/var/lib/mysql-files/` on Unix). I just put them there and copied them out by hand (you will need `sudo`, since they will by owned by the user mysql).

`mysql> SELECT something_interesting FROM table INTO /var/lib/mysql-files/out.txt;`

## Success Thresholding
Repos with less than 172 events over the entire course of 2015 are discarded automatically (using filtering in example workflow above). Frequency counts are ten extracted from the remaining repos.

`mysql> SELECT COUNT(*), repo, MONTH(created) AS created_month FROM events GROUP BY repo, created_month INTO OUTFILE 'relevant_counts';`

In order to be classified as successful it must beat the average number of commits for that month in our remaining data 6 months out of the year. This gives the following number of successful and unsuccessful repos:

8954 successful repos (6.410825517290757%)
130716 unsuccessful repos (93.58917448270925%)

Create a column for success (TRUE/FALSE) in the repos table of the database indicating the success of the repo

`./success.py -r year.pickle -t 5 success_counts/* -p`

